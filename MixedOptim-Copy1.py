#!/usr/bin/env python
# coding: utf-8

# In[1]:


import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, models, losses
from tensorflow.keras.models import Sequential
import matplotlib.pyplot as plt
import numpy as np
import PIL
import pathlib
import os
import re
import shutil
import string
import tqdm


# In[2]:


data_dir_image = r"C:\Users\leven\Desktop\tdkMixedTanul\hatvannegysorted"
data_dir_text = r"C:\Users\leven\Desktop\tdkMixedTanul\malwaretxttrainclasses"


# In[3]:


data_dir_image = pathlib.Path(data_dir_image)
image_count = len(list(data_dir_image.glob('*/*.png')))
print(image_count)
categories = np.array(os.listdir(data_dir_text))
categories

filenames = ["\\".join(str(path).split("\\")[-2:]).split('.')[0] for path in data_dir_image.glob('*/*.png')]
np.random.shuffle(filenames)
filenames


# In[4]:


val_split = 0.2

#parameters for the image
batch_size_image = 32
img_height = 64
img_width = 64

#text
batch_size_text = 32
seed = 123
max_features = 255
sequence_length = 64000
embedding_dim = 64


# In[5]:


def onehot_encode_category(cat_name):
    cat_index = np.argwhere(categories == cat_name)[0][0]
    result = np.zeros(len(categories))
    result[cat_index] = 1.
    return result

def train_data_generator():
    for fpath in filenames[:int(len(filenames) * (1. - val_split))]:
        img_path = str(data_dir_image) + "\\" + fpath + ".png"
        txt_path = str(data_dir_text) + "\\" + fpath + ".txt"
        
        img_load = tf.keras.utils.load_img(img_path, color_mode="grayscale", target_size=(img_height, img_width))
        img_load = np.array(img_load)
        img_load = img_load.reshape((img_height, img_width, 1))
        img_load = tf.convert_to_tensor(img_load)
        
        with open(txt_path) as file:
            txt_load = file.read().replace('\n', ' ')[:-1]
            file.close()
        txt_list = txt_load.split(' ')
        txt_vectorized = tf.strings.to_number(txt_list, out_type=tf.dtypes.int32)
        
        label = fpath.split('\\')[0]
        label_onehot = tf.convert_to_tensor(onehot_encode_category(label))

        yield (img_load.numpy(), txt_vectorized.numpy()), label_onehot.numpy()

def val_data_generator():
    for fpath in filenames[int(len(filenames) * (1. - val_split)):]:
        img_path = str(data_dir_image) + "\\" + fpath + ".png"
        txt_path = str(data_dir_text) + "\\" + fpath + ".txt"
        
        img_load = tf.keras.utils.load_img(img_path, color_mode="grayscale", target_size=(img_height, img_width))
        img_load = np.array(img_load)
        img_load = img_load.reshape((img_height, img_width, 1))
        img_load = tf.convert_to_tensor(img_load)
        
        with open(txt_path) as file:
            txt_load = file.read().replace('\n', ' ')[:-1]
            file.close()
        txt_list = txt_load.split(' ')
        txt_vectorized = tf.strings.to_number(txt_list, out_type=tf.dtypes.int32)
        
        label = fpath.split('\\')[0]
        label_onehot = tf.convert_to_tensor(onehot_encode_category(label))

        yield (img_load.numpy(), txt_vectorized.numpy()), label_onehot.numpy()


# In[6]:


train_data_img = []
train_data_txt = []
train_data_labels = []
val_data_img = []
val_data_txt = []
val_data_labels = []


# In[7]:


with tqdm.tqdm(total=int(len(filenames) * (1 - val_split))) as pbar:
    for (img, txt), label in iter(train_data_generator()):
        img = img.astype(np.uint8)
        txt = txt.astype(np.uint8)
        train_data_img.append(img)
        train_data_txt.append(txt)
        train_data_labels.append(label)
        pbar.update(1)
        
with tqdm.tqdm(total=int(len(filenames) * val_split)) as pbar:
    for (img, txt), label in iter(val_data_generator()):
        img = img.astype(np.uint8)
        txt = txt.astype(np.uint8)
        val_data_img.append(img)
        val_data_txt.append(txt)
        val_data_labels.append(label)
        pbar.update(1)


# In[8]:


#numpy array from train data
train_data_img = np.array(train_data_img)
train_data_txt = np.array(train_data_txt)
train_data_labels = np.array(train_data_labels)

#numpy array from validation data
val_data_img = np.array(val_data_img)
val_data_txt = np.array(val_data_txt)
val_data_labels = np.array(val_data_labels)

train_data_img
val_data_img


# In[9]:


model_text = tf.keras.Sequential([
  layers.Embedding(max_features + 1, embedding_dim),
  layers.Dropout(0.2),
  layers.GlobalAveragePooling1D(),
  layers.Dropout(0.2),
  layers.Dense(128, activation="relu"),
  layers.Dense(16, activation="relu")])

model_image = Sequential([
  layers.Rescaling(1./255, input_shape=(img_height, img_width, 1)),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(16, activation='relu')
])


# In[10]:


model_text.summary()


# In[11]:


model_image.summary()


# In[12]:


from tensorflow.keras.layers import concatenate
combinedInput = concatenate([model_text.output, model_image.output])


# In[13]:


# x = layers.Dense(64, activation="relu")(combinedInput)
x = layers.Dense(32, activation="sigmoid")(combinedInput)
x = layers.Dropout(0.2)(x)
x = layers.Dense(16, activation="relu")(x)
x = layers.Dense(9, activation="softmax")(x)

model_Mixed = tf.keras.Model(inputs = [model_text.input, model_image.input], outputs = x)

model_Mixed.compile(loss=losses.CategoricalCrossentropy(from_logits=False),
              optimizer='adam',
              metrics=['accuracy'])

model_Mixed.summary()


# In[14]:


gpus = tf.config.list_physical_devices('GPU')
if gpus:
  # Restrict TensorFlow to only use the first GPU
  try:
    tf.config.set_visible_devices(gpus[0], 'GPU')
    logical_gpus = tf.config.list_logical_devices('GPU')
    print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPU")
  except RuntimeError as e:
    # Visible devices must be set before GPUs have been initialized
    print(e)


# In[15]:


from sklearn.utils.class_weight import compute_class_weight
y_integers = np.argmax(train_data_labels, axis=1)
class_weights = compute_class_weight('balanced', classes=np.unique(y_integers), y=y_integers)
class_weights_dict = dict(enumerate(class_weights))
print(class_weights_dict)


# In[21]:


epochs = 50

history = model_Mixed.fit(
    x=[train_data_txt, train_data_img], y=train_data_labels,
    class_weight=class_weights_dict,
    validation_data=[(val_data_txt, val_data_img), val_data_labels],
    epochs=epochs
)


# In[17]:


y_pred = model_Mixed.predict((val_data_txt, val_data_img))
predicted_categories = tf.argmax(y_pred, axis=1)
true_categories = tf.argmax(val_data_labels, axis=1)
print(predicted_categories)
print(true_categories)


# In[18]:


from sklearn.metrics import confusion_matrix
cf = confusion_matrix(true_categories, predicted_categories, normalize='true')
cf
import seaborn as sn
import pandas as pd

columns = categories
table_data = pd.DataFrame(cf, columns, columns)
sn.set(font_scale=1.2)
sn.heatmap(table_data, annot=True, annot_kws={"size": 8}) # font size
plt.xlabel("Predicted class")
plt.ylabel("True class")

plt.show()


# In[19]:


acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()


# In[22]:


tf.keras.utils.plot_model(model_Mixed, to_file="bytemodell.png", show_shapes=True)


# In[ ]:




