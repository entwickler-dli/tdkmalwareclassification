import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, models, losses
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import concatenate
import numpy as np
import pathlib
import os
import re
import shutil
import string
import tqdm
import PIL.Image


# In[2]:

#
# def custom_standardization(input_data):
#     lowercase = tf.strings.lower(input_data)
#     stripped_html = tf.strings.regex_replace(lowercase, '\r', ' ')
#     stripped_html = tf.strings.regex_replace(lowercase, '\n', ' ')
#     return tf.strings.regex_replace(stripped_html,
#                                     '[%s]' % re.escape(string.punctuation),
#                                     '')
#
#
# # In[3]:
#
#
# def vectorize_text(text, label):
#     text = tf.expand_dims(text, -1)
#     return vectorize_layer(text), label
#

# In[4]:


data_dir_image = r"C:\Users\leven\Desktop\tdkMixedTanul\hatvannegysorted"
data_dir_text = r"C:\Users\leven\Desktop\tdkMixedTanul\malwaretxttrainclasses"

# In[5]:


data_dir_image = pathlib.Path(data_dir_image)
image_count = len(list(data_dir_image.glob('*/*.png')))
print(image_count)
categories = np.array(os.listdir(data_dir_text))
categories

# In[6]:


filenames = ["\\".join(str(path).split("\\")[-2:]).split('.')[0] for path in data_dir_image.glob('*/*.png')]
np.random.shuffle(filenames)
filenames

# In[7]:


val_split = 0.2

# parameters for the image
batch_size_image = 32
img_height = 64
img_width = 64

# text
batch_size_text = 32
seed = 123
max_features = 255
sequence_length = 64000
embedding_dim = 72


# In[8]:


def onehot_encode_category(cat_name):
    cat_index = np.argwhere(categories == cat_name)[0][0]
    result = np.zeros(len(categories))
    result[cat_index] = 1.
    return result


def train_data_generator():
    #     for _ in range(len(filenames)):
    #         fpath = np.random.choice(filenames[:int(len(filenames) * (1. - val_split))])
    for fpath in filenames[:int(len(filenames) * (1. - val_split))]:
        img_path = str(data_dir_image) + "\\" + fpath + ".png"
        txt_path = str(data_dir_text) + "\\" + fpath + ".txt"

        img_load = tf.keras.utils.load_img(img_path, color_mode="grayscale", target_size=(img_height, img_width))
        img_load = np.array(img_load)
        img_load = img_load.reshape((img_height, img_width, 1))
        img_load = tf.convert_to_tensor(img_load)

        with open(txt_path) as file:
            txt_load = file.read().replace('\n', ' ')[:-1]
            file.close()
        txt_list = txt_load.split(' ')
        txt_vectorized = tf.strings.to_number(txt_list, out_type=tf.dtypes.int32)

        label = fpath.split('\\')[0]
        label_onehot = tf.convert_to_tensor(onehot_encode_category(label))

        #         yield {"rescaling_2_input": img_load, "embedding_1_input": txt_vectorized}, label_onehot
        yield (img_load.numpy(), txt_vectorized.numpy()), label_onehot.numpy()


def val_data_generator():
    #     for _ in range(len(filenames)):
    #         fpath = np.random.choice(filenames[int(len(filenames) * (1. - val_split)):])
    for fpath in filenames[int(len(filenames) * (1. - val_split)):]:
        img_path = str(data_dir_image) + "\\" + fpath + ".png"
        txt_path = str(data_dir_text) + "\\" + fpath + ".txt"

        img_load = tf.keras.utils.load_img(img_path, color_mode="grayscale", target_size=(img_height, img_width))
        img_load = np.array(img_load)
        img_load = img_load.reshape((img_height, img_width, 1))
        img_load = tf.convert_to_tensor(img_load)

        with open(txt_path) as file:
            txt_load = file.read().replace('\n', ' ')[:-1]
            file.close()
        txt_list = txt_load.split(' ')
        txt_vectorized = tf.strings.to_number(txt_list, out_type=tf.dtypes.int32)

        label = fpath.split('\\')[0]
        label_onehot = tf.convert_to_tensor(onehot_encode_category(label))

        #         yield {"rescaling_2_input": img_load, "embedding_1_input": txt_vectorized}, label_onehot
        yield (img_load.numpy(), txt_vectorized.numpy()), label_onehot.numpy()


# In[9]:


train_data_img = []
train_data_txt = []
train_data_labels = []
with tqdm.tqdm(total=int(len(filenames) * (1 - val_split))) as pbar:
    for (img, txt), label in iter(train_data_generator()):
        img = img.astype(np.uint8)
        txt = txt.astype(np.uint8)
        train_data_img.append(img)
        train_data_txt.append(txt)
        train_data_labels.append(label)
        pbar.update(1)

train_data_img = np.array(train_data_img)
train_data_txt = np.array(train_data_txt)
train_data_labels = np.array(train_data_labels)

# In[10]:


val_data_img = []
val_data_txt = []
val_data_labels = []
with tqdm.tqdm(total=int(len(filenames) * val_split)) as pbar:
    for (img, txt), label in iter(val_data_generator()):
        img = img.astype(np.uint8)
        txt = txt.astype(np.uint8)
        val_data_img.append(img)
        val_data_txt.append(txt)
        val_data_labels.append(label)
        pbar.update(1)

val_data_img = np.array(val_data_img)
val_data_txt = np.array(val_data_txt)
val_data_labels = np.array(val_data_labels)

# In[12]:


model_text = tf.keras.Sequential([
    layers.Embedding(max_features + 1, embedding_dim),
    layers.Dropout(0.2),
    layers.GlobalAveragePooling1D(),
    layers.Dropout(0.2),
    layers.Dense(32, activation="relu"),
    layers.Dense(24, activation="relu"),
    layers.Dense(16, activation="relu")])

model_text.summary()

# In[13]:


model_image = tf.keras.Sequential([
    layers.Rescaling(1. / 255, input_shape=(img_height, img_width, 1)),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(256, activation='relu'),
    layers.Dense(16)
])

model_image.summary()

# In[14]:


combinedInput = concatenate([model_text.output, model_image.output])

# In[15]:


x = layers.Dense(16, activation="relu")(combinedInput)
x = layers.Dense(9)(x)

# In[16]:


model_Mixed = tf.keras.Model(inputs=[model_text.input, model_image.input], outputs=x)

# In[17]:


model_Mixed.compile(loss=losses.CategoricalCrossentropy(from_logits=True),
                    optimizer='adam',
                    metrics=tf.metrics.CategoricalAccuracy())

# In[18]:


model_Mixed.summary()

# In[ ]:


epochs = 10

history = model_Mixed.fit(x=[train_data_txt, train_data_img], y=train_data_labels,
                          validation_data=[(val_data_txt, val_data_img), val_data_labels],
                          epochs=epochs)

# In[ ]:




